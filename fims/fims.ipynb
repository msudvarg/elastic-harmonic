{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# S.VI.B: FIMS\n",
    "\n",
    "This notebook plots and analyzes the results shown in Figure 7a -- 7f of Section VI.B: \"FIMS.\"\n",
    "\n",
    "To simplify reproducability of the artifact, we have created three slightly-modified copies of the FIMS software pipeline presented in:\n",
    "\n",
    "> D. Wang, J. Zhang, J. Buhler, and J. Wang, “Real-time analysis of aerosol size distributions with the fast integrated mobility spectrometer (FIMS),” in 41st Conference of American Association for Aerosol Research (AAAR), Oct. 2023. [Online]. Available: https://aaarabstracts.com/2023/view_abstract.php?pid=752 \n",
    "\n",
    "These include:\n",
    "* `fims_execution_time_profile`: Obtains execution times with `getrusage()` for each job iteration of all tasks.\n",
    "* `fims_elasticity_values_assignment`: Obtains outputs using custom periods (stacking images, interpolating HK data, using different particle time bins for inversion) to assess impact of changing rates on result quality, allowing us to obtain elasticity constants.\n",
    "* `fims_evaluate_scalability`: Runs FIMS using custom periods in a resource-constrained environment and measures job response times to assess whether all deadlines are met.\n",
    "\n",
    "Other files include:\n",
    "* `over_head`: A custom interference task to evaluate FIMS (`fims_evaluate_scalability`) in a simulated resource-constrained environment.\n",
    "* `raw_data`: Input data (particle images captured from the FIMS camera, HK data captured from FIMS sensors) to test the pipeline.\n",
    "\n",
    "## Build and Prepare System\n",
    "\n",
    "### Enable `sudo` Commands from Jupyter Notebook\n",
    "\n",
    "Several of the scripts in this notebook have to be run as `root`, e.g., to set real-time priorities. We use the `getpass` Python module to prompt for a password, then invoke `sudo` using `os.system`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "password = getpass.getpass()\n",
    "def run_sudo(cmd) :\n",
    "    return os.system('echo %s | sudo -S %s' % (password, cmd))\n",
    "if run_sudo(\"su\") != 0 :\n",
    "    print(\"Entered password incorrect! Please run this cell again.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install OpenCV\n",
    "\n",
    "***Note: The following script can be skipped on the provided virtual machine***\n",
    "\n",
    "Refer to the OpenCV installation tutorial for more detailed instrutions:\n",
    "https://docs.opencv.org/4.7.0/d7/d9f/tutorial_linux_install.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Download and unpack sources\n",
    "wget -O opencv.zip https://github.com/opencv/opencv/archive/4.7.0.zip\n",
    "wget -O opencv_contrib.zip https://github.com/opencv/opencv_contrib/archive/4.7.0.zip\n",
    "unzip opencv.zip\n",
    "unzip opencv_contrib.zip\n",
    "\n",
    "# Create build directory and switch into it\n",
    "mkdir -p build && cd build\n",
    "\n",
    "# Configure\n",
    "cmake -DOPENCV_EXTRA_MODULES_PATH=../opencv_contrib-4.7.0/modules ../opencv-4.7.0\n",
    "\n",
    "# Build\n",
    "make -j4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install\n",
    "run_sudo(\"make install\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compile FIMS Code\n",
    "\n",
    "***Note: The following script can be skipped on the provided virtual machine***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "# Compile Code for Profiling Execution Times\n",
    "cd fims_execution_time_profile\n",
    "mkdir -p build && cd build\n",
    "cmake ..\n",
    "make -j4\n",
    "\n",
    "# Compile Code for Assigning Elasticity Values\n",
    "cd ../../fims_elasticity_values_assignment\n",
    "mkdir -p build && cd build\n",
    "cmake ..\n",
    "make -j4\n",
    "\n",
    "# Compile Code for Evaluation of Online Adjustment\n",
    "cd ../../fims_evaluate_scalability\n",
    "mkdir -p build && cd build\n",
    "cmake ..\n",
    "make -j4\n",
    "\n",
    "# Compile Code for Interference Task\n",
    "cd ../../over_head\n",
    "make"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Disable Throttling\n",
    "\n",
    "CPU throttling and real-time scheduler throttling were disabled for our experiments. The following settings were used.\n",
    "\n",
    "***Note that this is optional, and not required for correct functionality (though is required to faithfully reproduce timing results on a real system).***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_sudo(\"sh -c 'echo -1 > /proc/sys/kernel/sched_rt_runtime_us'\")\n",
    "run_sudo(\"cpufreq-set -r -g performance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Profiling Execution Times\n",
    "\n",
    "We first profile the execution times of each FIMS pipeline task. We make calls to `getrusage()` when each job completes, measuring the total CPU time (user and system) consumed by the task since the end of the prior job. This accounts for execution of the task's function plus the overhead of context switching and timer handling. To capture worst-case conditional behavior, we force recalculation of the inversion matrix with each iteration of data inversion.\n",
    "\n",
    "The following code reproduces the results shown in Figure 7d -- 7f.\n",
    "\n",
    "### Obtain Execution Times\n",
    "\n",
    "We have already provided the results data in the files under `fims/fims_execution_time_profile/result_/execution_time_on_raspberrypi`. This can be reproduced by running the following.\n",
    "\n",
    "***Note that a single run takes around 20 minutes.***\n",
    "Provided data is from 10 runs each over two different aerosol datasets. To save time for the reviewers, the below code only performs a single run by default, though this can be controlled by changing the `run_times` variable.\n",
    "\n",
    "Input data are real images and sensor data obtained by the FIMS instrument, and are found in `fims/raw_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "def enter_folder(folder) :\n",
    "    # print(\"Currently in\", os.getcwd(), \"entering\", folder)\n",
    "    if os.getcwd()[-1*len(folder):] != folder :\n",
    "        os.chdir(folder)\n",
    "\n",
    "def leave_folder(folder) :\n",
    "    os.chdir(\"../\")\n",
    "    for _ in range(folder.count(\"/\")):\n",
    "        os.chdir(\"../\")\n",
    "    # print(\"Back in\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change this to perform additional runs\n",
    "run_times = 1\n",
    "\n",
    "# Function to rename the folder and create a new one\n",
    "def manage_folders(old_folder, new_folder, arg):\n",
    "    os.rename(old_folder, f\"{old_folder}_{arg}\")\n",
    "    os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "execution_time_working_folder = \"fims_execution_time_profile/result_\"\n",
    "executable = \"../build/fims\"  \n",
    "result_folder = \"run_time\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter execution time profiling folder\n",
    "enter_folder(execution_time_working_folder)\n",
    "\n",
    "# Create result directory\n",
    "os.makedirs(result_folder, exist_ok=True)\n",
    "\n",
    "for i in range(0, run_times, 1):  # define how many time to run the code\n",
    "    print(f'start running {i} ...')\n",
    "    if run_sudo(executable) == 0:\n",
    "        print(f\"Run {i+1}: Success\")\n",
    "        # Rename the old folder and create a new one for the next run\n",
    "        manage_folders(result_folder, result_folder, i)\n",
    "    else:\n",
    "        print(f\"Run {i+1}: Failure\")\n",
    "\n",
    "    time.sleep(1)  # Optional: pause for 1 second between runs\n",
    "\n",
    "leave_folder(execution_time_working_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are written into a collection of folders `fims/fims_execution_time_profile/result_/run_time_x`, with x indexing each run. Each file contains space-separated values corresponding to the execution time of each iteration of the corresponding task, in units of milliseconds. We use the times reported by `get_rusage()`, separately writing the system times (files ending in `_sys`) and user times (files ending in `_user`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and Analyze Results\n",
    "\n",
    "The following code plots Figure 7d--7f, showing execution time distributions. It also reports the worst observed execution times for each task.\n",
    "\n",
    "We provide the ability to either plot the original data or data newly-generated by the above steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_execution_times(data, data2) :\n",
    "    \n",
    "    for i in range(len(data)):\n",
    "        data[i] = data[i] + data2[i]\n",
    "\n",
    "    components = ['image_processing', 'data_inversion', 'hk_reading']\n",
    "    titles = ['(d) FIMS: Image', '(f) FIMS: Inversion', '(e) FIMS: HK']\n",
    "    maximums = [0,0,0]\n",
    "\n",
    "    # Define a palette\n",
    "    colors = sns.color_palette(\"deep\", 5)\n",
    "\n",
    "    # Use the fivethirtyeight style\n",
    "    plt.style.use('fivethirtyeight')\n",
    "\n",
    "    for i, d in enumerate(data):\n",
    "        fig, ax = plt.subplots(figsize=(4.6, 2.5))\n",
    "        ax.hist(d, bins=30, density=True, alpha=0.8, edgecolor='black', linewidth=1.5, color=colors[i])\n",
    "        ax.set_yscale('log')\n",
    "        ax.set_xlabel('Execution Time (ms)', )\n",
    "        ax.set_ylabel('Frequency')\n",
    "        plt.tight_layout()\n",
    "        fig.savefig(f'{components[i]}_execution_time.png')\n",
    "        fig.savefig(f'{components[i]}_execution_time.eps')\n",
    "        plt.show() \n",
    "        maximums[i] = np.max(data[i])\n",
    "        print(f'{titles[i]} (max {maximums[i]}ms)')\n",
    "    \n",
    "    return maximums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Original Results\n",
    "\n",
    "Plots results obtained for the paper from the files under `fims/fims_execution_time_profile/result_/execution_time_on_raspberrypi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter execution time profiling folder\n",
    "execution_time_working_folder = \"fims_execution_time_profile/result_/execution_time_on_raspberrypi\"\n",
    "enter_folder(execution_time_working_folder)\n",
    "\n",
    "# Process userspace time\n",
    "file_name = 'sparse'\n",
    "file_name2 = 'dense'\n",
    "path = ['/image_process_cost_user.txt', '/inversion_time_user.txt', '/hk_reading_time_user.txt']\n",
    "data = []\n",
    "for p in path:\n",
    "    data1 = []\n",
    "    for i in range(1, 11, 1):\n",
    "        file = 'run_time_' + file_name + '_' + str(i) + p\n",
    "        cur_data = np.loadtxt(file)\n",
    "        data1 = np.concatenate((data1, cur_data[1:]))\n",
    "        if p == '/image_process_cost_user.txt':\n",
    "            file = 'run_time_' + file_name2 + '_' + str(i) + p\n",
    "            cur_data = np.loadtxt(file)\n",
    "            data1 = np.concatenate((data1, cur_data[1:]))\n",
    "    data.append(data1)\n",
    "\n",
    "# Process system time\n",
    "path_sys = ['/image_process_cost_sys.txt', '/inversion_time_sys.txt', '/hk_reading_time_sys.txt']\n",
    "data2 = []\n",
    "for p in path_sys:\n",
    "    data1 = []\n",
    "    for i in range(1, 11, 1):\n",
    "        file = 'run_time_' + file_name + '_' + str(i) + p\n",
    "        cur_data = np.loadtxt(file)\n",
    "        data1 = np.concatenate((data1, cur_data[1:]))\n",
    "        if p == '/image_process_cost_sys.txt':\n",
    "            file = 'run_time_' + file_name2 + '_' + str(i) + p\n",
    "            cur_data = np.loadtxt(file)\n",
    "            data1 = np.concatenate((data1, cur_data[1:]))\n",
    "\n",
    "    data2.append(data1)\n",
    "\n",
    "# Plot\n",
    "maximums = plot_execution_times(data,data2)\n",
    "\n",
    "leave_folder(execution_time_working_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Newly-Obtained Results\n",
    "\n",
    "Plots new results obtained from running the above profiling steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter execution time profiling folder\n",
    "execution_time_working_folder = \"fims_execution_time_profile/result_\"\n",
    "enter_folder(execution_time_working_folder)\n",
    "\n",
    "# Process userspace time\n",
    "path = ['/image_process_time_user.txt', '/inversion_time_user.txt', '/hk_reading_time_user.txt']\n",
    "data = []\n",
    "for p in path:\n",
    "    data1 = []\n",
    "    for i in range(0, run_times, 1):\n",
    "        file = 'run_time_' + str(i) + p\n",
    "        cur_data = np.loadtxt(file)\n",
    "        data1 = np.concatenate((data1, cur_data[1:]))\n",
    "    data.append(data1)\n",
    "\n",
    "# Process system time\n",
    "path_sys = ['/image_process_time_sys.txt', '/inversion_time_sys.txt', '/hk_reading_time_sys.txt']\n",
    "data2 = []\n",
    "for p in path_sys:\n",
    "    data1 = []\n",
    "    for i in range(0, run_times, 1):\n",
    "        file = 'run_time_' + str(i) + p\n",
    "        cur_data = np.loadtxt(file)\n",
    "        data1 = np.concatenate((data1, cur_data[1:]))\n",
    "    data2.append(data1)\n",
    "\n",
    "# Plot\n",
    "plot_execution_times(data,data2)\n",
    "\n",
    "leave_folder(execution_time_working_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assigning Elasticity Values\n",
    "\n",
    "We next assign elasticity values by measuring the loss associated with adjusting task periods individually. To reflect the real instrument's behavior, when increasing the image processing period, we stack successive image frames; stacked images are found in `fims/raw_data/xstack`, where x denotes the number of frames to be stacked.\n",
    "\n",
    "The following code reproduces the results shown in Figure 7a -- 7c.\n",
    "\n",
    "### Obtain Loss Values\n",
    "\n",
    "We have already provided the results data in the files under `fims/fims_elasticity_values_assignment/result_`. This can be reproduced by running the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "\n",
    "def manage_files(old_file, arg1, arg2):\n",
    "    name_part, extension_part = os.path.splitext(old_file)\n",
    "    new_name = f\"{name_part}_{arg1}_{arg2}{extension_part}\"\n",
    "    os.rename(old_file, new_name)\n",
    "\n",
    "elastic_working_folder = \"fims_elasticity_values_assignment/result_\"\n",
    "executable = \"../build/fims\"  \n",
    "result_file = \"n_Dp_fixed.txt\"\n",
    "fims_config_file_path = '../configuration/config.json'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image Processing\n",
    "\n",
    "Obtain results for adjusting image processing periods from 100 to 1000 ms.\n",
    "\n",
    "***Note: This may take around 10 minutes to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter elasticity value folder\n",
    "enter_folder(elastic_working_folder) \n",
    "\n",
    "# Task Period Values\n",
    "T_image = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]\n",
    "\n",
    "for i in range(len(T_image)): \n",
    "    with open(fims_config_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data['image_processing_duration'] = T_image[i] \n",
    "        data['HK_reading_duration'] = 500\n",
    "        data['data_inversion_duration'] = 1000\n",
    "\n",
    "    with open(fims_config_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    if run_sudo(executable) == 0:\n",
    "        print(f\"Running program with image process duration {T_image[i]}: Success\")\n",
    "        manage_files(result_file, 'image', T_image[i])\n",
    "    else:\n",
    "        print(f\"Running program with image process duration {T_image[i]}: Fail\")\n",
    "    time.sleep(1)  # pause for 1 second between runs\n",
    "\n",
    "leave_folder(elastic_working_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HK Reading\n",
    "\n",
    "Obtain results for adjusting HK data reading periods from 500 to 5000 ms.\n",
    "\n",
    "***Note: This may take around 30 minutes to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter elasticity value folder\n",
    "enter_folder(elastic_working_folder) \n",
    "\n",
    "# Task Period Values\n",
    "T_HK = [500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "\n",
    "for i in range(len(T_HK)): \n",
    "    with open(fims_config_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data['HK_reading_duration'] = T_HK[i] \n",
    "        data['image_processing_duration'] = 100 \n",
    "        data['data_inversion_duration'] = 1000\n",
    "\n",
    "    with open(fims_config_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    if run_sudo(executable) == 0:\n",
    "        print(f\"Running program with hk reading duration {T_HK[i]}: Success\")\n",
    "        manage_files(result_file, 'hk', T_HK[i])\n",
    "    else:\n",
    "        print(f\"Running program with hk reading duration {T_HK[i]}: Fail\")\n",
    "    time.sleep(1)  # pause for 1 second between runs\n",
    "\n",
    "leave_folder(elastic_working_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Inversion\n",
    "\n",
    "Obtain results for adjusting data matrix inversion periods from 1 to 10 seconds.\n",
    "\n",
    "***Note: This may take around 30 minutes to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter elasticity value folder\n",
    "enter_folder(elastic_working_folder) \n",
    "\n",
    "# Task Period Values\n",
    "T_inversion = [1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000]\n",
    "\n",
    "for i in range(len(T_inversion)): \n",
    "    with open(fims_config_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "        data['data_inversion_duration'] = T_inversion[i] \n",
    "        data['image_processing_duration'] = 100 \n",
    "        data['HK_reading_duration'] = 500\n",
    "\n",
    "    with open(fims_config_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "\n",
    "    if run_sudo(executable) == 0:\n",
    "        print(f\"Running program with hk reading duration {T_inversion[i]}: Success\")\n",
    "        manage_files(result_file, 'inversion', T_inversion[i])\n",
    "    else:\n",
    "        print(f\"Running program with hk reading duration {T_inversion[i]}: Fail\")\n",
    "    time.sleep(1)  # pause for 1 second between runs\n",
    "\n",
    "leave_folder(elastic_working_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot and Analyze Results\n",
    "\n",
    "The following code plots Figure 7a--7c, deriving error for each tested task period, then fitting the error as a function of the square deviation in rate. It uses this to obtain and report the elasticity values.\n",
    "\n",
    "#### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "\n",
    "# Define Style for Plotting\n",
    "\n",
    "# Use the fivethirtyeight style\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "# Define a palette\n",
    "colors = sns.color_palette(\"deep\", 5)\n",
    "\n",
    "# Read CSV files\n",
    "def read_csv(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            row = list(map(float, line.strip().split(',')))\n",
    "            data.append(row)\n",
    "    return data\n",
    "\n",
    "# Read Text output\n",
    "def read_txt(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    matrix = []\n",
    "    row = []\n",
    "    for line in lines:\n",
    "        value = line.strip()\n",
    "        if value == \"NaN\":\n",
    "            matrix.append(row)\n",
    "            row = []\n",
    "        else:\n",
    "            row.append(float(value))\n",
    "\n",
    "    if row:\n",
    "        matrix.append(row)\n",
    "\n",
    "    matrix = np.array(matrix)\n",
    "    return matrix\n",
    "\n",
    "# Skip Rows for Larger Time Windows (used for larger inversion periods)\n",
    "def sum_every_n_rows(arr, n):\n",
    "    num_rows_to_keep = (len(arr) // n) * n\n",
    "    trimmed_arr = arr[:num_rows_to_keep]   \n",
    "    reshaped = trimmed_arr.reshape(-1, n, arr.shape[1])\n",
    "    return reshaped.sum(axis=1)\n",
    "\n",
    "# Compute FIMS Errors Based on Cosine Similarities\n",
    "def compute_errors(durations, n, filename) :\n",
    "\n",
    "    enter_folder(elastic_working_folder) \n",
    "\n",
    "    # Get ground truth   \n",
    "    n_fixed_array_matlab = [] \n",
    "    n_Dp_fixed_matlab = read_csv('n_Dp_fixed_mat.csv')\n",
    "    n_Dp_fixed_matlab = np.array(n_Dp_fixed_matlab, dtype=float)\n",
    "    if n > 0 :\n",
    "        n_fixed_array_matlab = np.nan_to_num(n_Dp_fixed_matlab)[:n]\n",
    "    else : # For larger inversion periods\n",
    "        n_Dp_fixed_matlab = np.nan_to_num(n_Dp_fixed_matlab)\n",
    "        for i in range(len(durations)):\n",
    "            n_fixed_array_matlab_ = sum_every_n_rows(n_Dp_fixed_matlab, i+1)\n",
    "            n_fixed_array_matlab.append(n_fixed_array_matlab_)\n",
    "\n",
    "    # Get degraded result data\n",
    "    n_fixed_array_cpp = []\n",
    "    for i in range(len(durations)):\n",
    "        n_Dp_fixed_cpp_path = filename + str(durations[i]) + '.txt'\n",
    "        n_Dp_fixed_cpp = read_txt(n_Dp_fixed_cpp_path)\n",
    "        n_Dp_fixed_cpp = np.array(n_Dp_fixed_cpp, dtype=float)        \n",
    "        if n > 0 :\n",
    "            n_Dp_fixed_cpp = np.nan_to_num(n_Dp_fixed_cpp)[:n]\n",
    "        else :\n",
    "            n_Dp_fixed_cpp = np.nan_to_num(n_Dp_fixed_cpp)\n",
    "        n_fixed_array_cpp.append(n_Dp_fixed_cpp)\n",
    "\n",
    "    # Compute cosine similarities for error\n",
    "\n",
    "    n_fixed_array_matlab_norm = []\n",
    "    n_fixed_array_matlab_sum = []\n",
    "    \n",
    "    if n > 0: # For fixed inversion periods\n",
    "        n_fixed_array_matlab_sum = np.sum(n_fixed_array_matlab, axis=1)[:, np.newaxis]\n",
    "        n_fixed_array_matlab_norm = np.where(n_fixed_array_matlab_sum!=0, n_fixed_array_matlab / n_fixed_array_matlab_sum, np.nan)\n",
    "\n",
    "    n_fixed_array_cpp_norm = []\n",
    "    cos_similarities = []\n",
    "    mean_cos_similarities = []\n",
    "\n",
    "    for i in range(len(durations)):\n",
    "        n_fixed_array_cpp_sum = np.sum(n_fixed_array_cpp[i], axis=1)[:, np.newaxis]\n",
    "        n_fixed_array_cpp_norm_ = np.where(n_fixed_array_cpp_sum!=0, n_fixed_array_cpp[i] / n_fixed_array_cpp_sum, np.nan)\n",
    "        n_fixed_array_cpp_norm_ = np.nan_to_num(n_fixed_array_cpp_norm_) + 1e-15\n",
    "        n_fixed_array_cpp_norm.append(n_fixed_array_cpp_norm_)\n",
    "\n",
    "        if n > 0 : # For fixed inversion periods         \n",
    "            similarities = [cosine_similarity(n_fixed_array_matlab_norm[j].reshape(1, -1), n_fixed_array_cpp_norm[i][j].reshape(1, -1))[0][0] for j in range(n_fixed_array_cpp_norm[i].shape[0])]\n",
    "        else : # For larger inversion periods\n",
    "            n_fixed_array_matlab_sum = np.sum(n_fixed_array_matlab[i], axis=1)[:, np.newaxis]\n",
    "            n_fixed_array_matlab_norm_ = np.where(n_fixed_array_matlab_sum!=0, n_fixed_array_matlab[i] / n_fixed_array_matlab_sum, np.nan)\n",
    "            n_fixed_array_matlab_norm_ = np.nan_to_num(n_fixed_array_matlab_norm_) + 1e-15\n",
    "            n_fixed_array_matlab_norm.append(n_fixed_array_matlab_norm_)\n",
    "            similarities = [cosine_similarity(n_fixed_array_matlab_norm[i][j].reshape(1, -1), n_fixed_array_cpp_norm[i][j].reshape(1, -1))[0][0] for j in range(n_fixed_array_cpp_norm[i].shape[0])]\n",
    "            \n",
    "\n",
    "        cos_similarities.append(similarities)\n",
    "        mean_similarities = np.mean(similarities)\n",
    "        mean_cos_similarities.append(mean_similarities)\n",
    "    \n",
    "    leave_folder(elastic_working_folder)\n",
    "\n",
    "    return mean_cos_similarities\n",
    "\n",
    "def fit_and_plot(mean_cos_similarities, wcet) :\n",
    "\n",
    "    # Fit error to square of rate deviation\n",
    "\n",
    "    R = 1 / durations * 1000\n",
    "    R_max = R[0]\n",
    "    X = (R_max - R) ** 2\n",
    "    Y = (mean_cos_similarities[0] - mean_cos_similarities) * 1000\n",
    "\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "\n",
    "    slope, intercept = np.polyfit(X, Y, 1)\n",
    "\n",
    "    # Plot\n",
    "\n",
    "    plt.figure(figsize=(4.6, 2.5))\n",
    "    plt.scatter(X, Y, color='blue')\n",
    "    plt.plot(X, slope * X + intercept, color='red', label='Best Fit')\n",
    "    plt.xlabel(r'$(R^{max} - R)^2$') \n",
    "    plt.ylabel('Error')\n",
    "    plt.tight_layout()\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Compute Elasticity\n",
    "    e = wcet ** 2 / slope / 1000\n",
    "    print(f'Elasticity Constant: {e}')\n",
    "    return e\n",
    "\n",
    "def derive_elasticity(durations, n, filename, wcet) :    \n",
    "    return fit_and_plot(compute_errors(durations, n, filename), wcet)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 7a\n",
    "\n",
    "The following code plots Figure 7a, showing the derived error for each image processing period (from 100--1000) and fitting as a function of square deviation in rate. It also reports the derived elasticity according to Equation 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = np.array([100, 200, 300, 400, 500, 600, 700, 800, 900, 1000])\n",
    "n = 1193\n",
    "filename = \"n_Dp_fixed_image_\"\n",
    "wcet_image = maximums[0]\n",
    "e_image = derive_elasticity(durations, n, filename, wcet_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 7b\n",
    "\n",
    "The following code plots Figure 7b, showing the derived error for each HK data reading period (from 500--5000) and fitting as a function of square deviation in rate. It also reports the derived elasticity according to Equation 23."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = np.array([500, 1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000])\n",
    "n = 1195\n",
    "filename = \"n_Dp_fixed_hk_\"\n",
    "wcet_hk = maximums[2]\n",
    "e_hk = derive_elasticity(durations, n, filename, wcet_hk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Figure 7c\n",
    "\n",
    "The following code plots Figure 7c, showing the derived error for each data inversion period (from 1000--10000) and fitting as a function of square deviation in rate. It also reports the derived elasticity according to Equation 23. Note that this is more involved because ... TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "durations = np.array([1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000])\n",
    "n = 0\n",
    "filename = \"n_Dp_fixed_inversion_\"\n",
    "wcet_inversion = maximums[1]\n",
    "e_inversion = derive_elasticity(durations, n, filename, wcet_inversion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Scalability\n",
    "\n",
    "To test under tighter utilization constraints, we run a highest priority interference task that limits CPU utilization by FIMS: it registers an interval timer with a period of 50 ms and spins for a programmable length of time. We run FIMS concurrently with the interference task using different busy loop durations, adjusting the FIMS task periods according to our harmonic elastic model. We then measure each FIMS job’s latency (elapsed wallclock time) from task release to completion.\n",
    "\n",
    "The following code reproduces the results shown in the last table of Section VI.B.\n",
    "\n",
    "### Obtain Periods According to Elastic Model\n",
    "\n",
    "First, build the wrapper program around the elastic harmonic library to obtain the periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "%%sh\n",
    "cd ..\n",
    "make fims_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then execute, supplying the WCET and E values already obtained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command = f\"../fims_periods {wcet_image} {e_image} {wcet_hk} {e_hk} {wcet_inversion} {e_inversion}\"\n",
    "os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Scalability Experiments\n",
    "\n",
    "We have already provided the results data in the files under `fims/fims_evaluate_scalability/result_/result_on_raspberrypi`. This can be reproduced by running the following.\n",
    "\n",
    "First, update periods according to the assigned values from the elastic model, based on above output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_image = [100, 115, 147, 222, 458]\n",
    "T_HK = [500, 575, 881, 3325, 3205]\n",
    "T_inv = [1000, 2298, 9682, 9973, 9615]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run FIMS alongside the interference process.\n",
    "\n",
    "***Note: This may take around 2 hours to run.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import subprocess\n",
    "import os\n",
    "import signal\n",
    "import time\n",
    "import psutil\n",
    "import json\n",
    "\n",
    "# Kill subprocess and children\n",
    "def kill_process_tree(pid, including_parent=True):  \n",
    "    parent = psutil.Process(pid)\n",
    "    for child in parent.children(recursive=True):\n",
    "        child.kill()\n",
    "    if including_parent:\n",
    "        parent.kill()\n",
    "\n",
    "# Function to run the C++ program\n",
    "def run_cpp_programs(executable1, executable2, args2):\n",
    "    cmd1 = f\"echo {password} | sudo -S {executable1}\"\n",
    "    cmd2 = f\"echo {password} | sudo -S {executable2} {args2}\"\n",
    "\n",
    "    proc1 = subprocess.Popen(cmd1, shell=True)\n",
    "    proc2 = subprocess.Popen(cmd2, shell=True)\n",
    "\n",
    "    # Wait for proc1 to complete\n",
    "    proc1.communicate()\n",
    "\n",
    "    # Kill proc2 when proc1 completes\n",
    "    kill_process_tree(proc2.pid)\n",
    "\n",
    "    return proc1.returncode == 0\n",
    "\n",
    "# Function to rename the folder and create a new one\n",
    "def manage_folders(old_folder, new_folder, arg):\n",
    "    # Rename the folder\n",
    "    os.rename(old_folder, f\"{old_folder}_{arg}\")\n",
    "\n",
    "    # Create a new folder for the next run\n",
    "    os.makedirs(new_folder, exist_ok=True)\n",
    "\n",
    "\n",
    "scalability_working_folder = \"fims_evaluate_scalability/result_\"\n",
    "fims_executable = \"../build/fims\"\n",
    "overhead_executable = '../../over_head/overhead'\n",
    "overhead_exe = 'overhead'\n",
    "result_folder = 'run_time'\n",
    "fims_config_file_path = '../configuration/config.json'\n",
    "\n",
    "# Utilizations to test\n",
    "utilization_D = [0.5, 0.4, 0.3, 0.2, 0.1]\n",
    "\n",
    "enter_folder(scalability_working_folder)\n",
    "\n",
    "#choose the dataset you want to run by changing the range\n",
    "for i in range(len(utilization_D)): \n",
    "    with open(fims_config_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    data['HK_reading_duration'] = T_HK[i]  \n",
    "    data['image_processing_duration'] = T_image[i] \n",
    "    data['data_inversion_duration'] = T_inv[i] \n",
    "\n",
    "    with open(fims_config_file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=4)\n",
    "    \n",
    "    overhead_utilization = 1 - utilization_D[i]\n",
    "    if run_cpp_programs(fims_executable, overhead_executable, str(overhead_utilization)):\n",
    "        print(f\"Running program with available utilization {utilization_D[i]}: Success\")\n",
    "        manage_folders(result_folder, result_folder, utilization_D[i])\n",
    "    else:\n",
    "        print(f\"Running program with available utilization {utilization_D[i]}: Failure\")\n",
    "\n",
    "\n",
    "    time.sleep(1)  # pause for 1 second between runs\n",
    "\n",
    "leave_folder(scalability_working_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results\n",
    "\n",
    "The following parses the data in `fims/fims_evaluate_scalability/result_/result_on_raspberrypi` to produce the results shown in the last table of Section VI.B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enter_folder(\"fims_evaluate_scalability/result_/result_on_raspberrypi\")\n",
    "\n",
    "utilizations = [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "components=['/image_process_cost.txt', '/hk_reading_time.txt', '/inversion_time.txt']\n",
    "\n",
    "data = []\n",
    "for component in components:\n",
    "    component_data = []\n",
    "    for utilization in utilizations:\n",
    "        data1 = []\n",
    "        for i in range(1, 4, 1):\n",
    "            file = 'run_time_' + str(utilization) + '_' + str(i) + component\n",
    "            cur_data = np.loadtxt(file)\n",
    "            data1 = np.concatenate((data1, cur_data[1:]))\n",
    "        component_data.append(data1)\n",
    "    data.append(component_data)\n",
    "\n",
    "\n",
    "components=['image_process', 'hk_reading', 'data_inversion']\n",
    "for i in range(len(data)):\n",
    "    for j in range(len(data[i])):\n",
    "        maximum = np.max(data[i][j])\n",
    "        print(f'{components[i]} utilization {utilizations[j]} Max Time: {maximum}')\n",
    "\n",
    "leave_folder(\"fims_evaluate_scalability/result_/result_on_raspberrypi\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
